\documentclass[a4paper,12pt]{article}
\usepackage{amsmath, amssymb, hyperref, listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{textgreek}

\geometry{margin=1in}
\setlength\parindent{0pt}

\title{CoringTreespotters Model notes}
\author{Christophe}
\date{\today}


\begin{document}

\maketitle

\section*{15 April 2025}
Quick notes after meeting with Victor where we talked about preliminary steps of building a model to answer the question of how juvenile trees respond to climate change. I would like to approach this as a recruitement capacity of tree saplings and how their growth varies across years and how it's impacted by temperature and growing season length (or maybe not length... more on that below)

\section{Come up with a model}

\subsection*{What are my predictors?}
We discussed which of the following would be the most relevant variable that relates to growth:
\begin{enumerate}
	\item \textbf{Growing Degree Days (GDD)} which would be calculated between the budburst and budset date of each individual. 
	\item \textbf{Growing Season Length (GSL)} which could be calculated by:
	\begin{enumerate}
		\item Substracting budset DOY by budburst DOY or 
		\item By taking the number of days of each year when the mean (or max/min) was above 5$^{\circ}$C (or maybe the nb of consecutive days when the temperature was above 5 or something like that).
	\end{enumerate}
\end{enumerate}

\par
\textbf{Preliminary model} \\ 
\begin{align}
	\log (w_{i,t}) \sim \text{normal}(\text{X}, \sigma) \\
	\alpha + \alpha_{sp[i]} + \beta_{sp[i]} X_{i,t} + \cdots
\end{align} 
What kind of pooling? Partial pooling since they are all within the same family?\\

\section{Simulate data}
I arbitrarily decided to sum up th GDD between 100 and 250 days for now in order to simplify data simulation. \\
yhat: combine the effects that I generated from the grand mean, the year, species and individual effect deviation effect from the mean, + error returns my yhat. That returns the observed outcome which is composed of multiple nested effects. 

\section{Set your priors}

\section{Run model on empirical data}

\section{Perform retrodictive checks using the model fit to your empirical data}

\section{Mis}

\newpage
\textit{Below I will be stating the obvious because I want to make sure I understand the notations and stuff. The steps will be notes and thoughts that I have when I code my model in R.}

\subsection*{Common Bayesian Notation and other definitions} % found online

\begin{itemize}
  \item $\theta$ --- Parameters of the model (can be a vector of parameters)
  \item Standard  -- How much the data are spread out. It quantifies the variation within a set of measurements
  \item Standard errror -- How much the measurements are spread around their means. It is the standard deviations of the means of the different treatments/individuals/whatever. It quantifies the variationin the means from multiple sets of measurements.
  \item $n_eff$ ---  N effective: Rhat denotes the split Rhat convergence diagnostic, which compares the results from different chains, reaching the value 1 at convergence and being higher than 1 for chains that have not fully mixed. $n_eff$ denotes the effective sample size of the Hamiltonian Monte Carlo run, which due to its iterative nature can be less or more than the efficiency of independent draws. Usually $n_eff$ > 400 is sufficient for reliable diagnostics and accurate parameter estimates.
  \item $mcse$ -- mcse stands for Monte Carlo standard error, which is the additional uncertainty due to using stochastic algorithm (not to be confused with the uncertainty presented by the posterior distribution). Monte Carlo standard error is negligible in all the examples in this book.
  \item $y$ --- Observed data
  \item $x$ --- Predictor variables or covariates
  \item $p(\theta)$ --- Prior distribution over parameters
  \item $p(y \mid \theta)$ --- Likelihood: the probability of the data given parameters
  \item $p(\theta \mid y)$ --- Posterior distribution: updated beliefs about parameters after observing data
  \item $p(y)$ --- Marginal likelihood or evidence: probability of the data under the model
  \item $p(y \mid x, \theta)$ --- Likelihood conditional on predictors
  \item $p(\theta, y)$ --- Joint distribution of parameters and data
  \item $\mu$ --- Mean (typically of a normal distribution or prior)
  \item $\sigma$ --- Standard deviation (scale of a distribution)
  \item $\tau$ --- Precision (inverse of variance: $\tau = 1/\sigma^2$)
  \item $\mathcal{N}(\mu, \sigma^2)$ --- Normal (Gaussian) distribution with mean $\mu$ and variance $\sigma^2$
  \item $\text{Beta}(\alpha, \beta)$ --- Beta distribution (common prior for probabilities)
  \item $\text{Gamma}(\alpha, \beta)$ --- Gamma distribution (common prior for positive variables)
  \item $\text{InvGamma}(\alpha, \beta)$ --- Inverse Gamma distribution (common prior for variances)
  \item $\text{Bernoulli}(p)$ --- Bernoulli distribution for binary data
  \item $\text{Binomial}(n, p)$ --- Binomial distribution
  \item $\text{Poisson}(\lambda)$ --- Poisson distribution for count data
  \item $\sim$ --- "Is distributed as" (e.g., $y \sim \mathcal{N}(\mu, \sigma^2)$)
  \item $\propto$ --- Proportional to (used in unnormalized posteriors: $p(\theta \mid y) \propto p(y \mid \theta) p(\theta)$)
\end{itemize}



%Where: 
%\begin{itemize}
%	\item 
%	\item alpha
%\end{itemize} 

\end{document}
